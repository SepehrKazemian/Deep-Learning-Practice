{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import decimal\n",
    "# decimal.getcontext().prec = 1000\n",
    "# np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network_initialization(num_input, hidden_units, num_classes, x_train):\n",
    "    #number of classes is 10 --> 0-9\n",
    "    #number of input features are 28 * 28\n",
    "    #xTw + b ==> x = [1, number of features][number of features, number of class]\n",
    "    # ==> [1, number of classes] <== b\n",
    "    w1 = np.random.rand(num_input, hidden_units)\n",
    "    b1 = np.random.rand(1, num_classes)\n",
    "    w2 = np.random.rand(hidden_units, hidden_units)\n",
    "    b2 = np.random.rand(1, hidden_units)\n",
    "    w_out = np.random.rand(hidden_units, num_classes)\n",
    "    b_out = np.random.rand(1, num_classes)\n",
    "    param = {}\n",
    "    param[\"w1\"] = w1\n",
    "    param[\"w2\"] = w2\n",
    "    param[\"w_out\"] = w_out\n",
    "    param[\"b1\"] = b1\n",
    "    param[\"b2\"] = b2\n",
    "    param[\"b_out\"] = b_out\n",
    "                     \n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHot(y):\n",
    "    m = y.shape[0]\n",
    "    OH = scipy.sparse.csr_matrix((np.ones(m), (y, np.array(range(m)))))\n",
    "    OH = np.array(OH.todense()).T\n",
    "    return OH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_preparation(x_train, x_test, y_train, y_test):\n",
    "    hyperparam = {}\n",
    "    hyperparam[\"num_epochs\"] = 1000\n",
    "    hyperparam[\"batch_size\"] = 64\n",
    "    hyperparam[\"num_samples\"] = x_train.shape[0]\n",
    "    hyperparam[\"mini_BGD\"] = False\n",
    "    hyperparam[\"num_batch\"] = int(hyperparam[\"num_samples\"] / hyperparam[\"batch_size\"])\n",
    "    hyperparam[\"learning_rate\"] = 0.001\n",
    "    last_batch = hyperparam[\"num_samples\"] % hyperparam[\"batch_size\"]\n",
    "\n",
    "    y_train = oneHot(y_train)\n",
    "    y_test = oneHot(y_test)\n",
    "\n",
    "    loss_epoch_report = 0\n",
    "    loss_batch_report = 0\n",
    "    x_train = x_train.reshape((x_train.shape[0], x_train.shape[1] * x_train.shape[2]))\n",
    "    x_test = x_test.reshape((x_test.shape[0], x_test.shape[1] * x_test.shape[2]))\n",
    "    return x_train, x_test, y_train, y_test, hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logloss(y_hat, y):\n",
    "    eps = np.finfo(float).eps\n",
    "    predictions = np.clip(y_hat, eps, 1 - eps)\n",
    "#     print(\"q\")\n",
    "    predictions /= np.sum(predictions)[np.newaxis]\n",
    "    rows = y.shape[0]\n",
    "    vsota = np.sum(y * np.log(predictions))\n",
    "#     print(\"l\")\n",
    "    value = (-1.0 / rows * vsota)\n",
    "#     print(\"c\")\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "#     z = np.exp(z - np.max(z, axis = 1)[:, np.newaxis])\n",
    "#     z = z / np.sum(z, axis = 1)[:, np.newaxis] #normalizing for softmax\n",
    "    a = np.exp(z) / np.sum(np.exp(z), axis = 1)[:, np.newaxis]\n",
    "    a_output = np.zeros_like(a)\n",
    "    a_output[np.arange(len(a)), a.argmax(1)] = 1\n",
    "    return a_output, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "def dReLU(x):\n",
    "    return 1. * (x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_data()\n",
    "from sklearn import preprocessing\n",
    "param = network_initialization(784, 10, 10, x_train)\n",
    "\n",
    "x_train, x_test, y_train, y_test, hyperparam = test_preparation(\n",
    "    x_train, x_test, y_train, y_test)\n",
    "\n",
    "# print(x_train[0])\n",
    "x_train = x_train / np.sum(x_train, axis = 1)[:, np.newaxis]\n",
    "# x_train.shape\n",
    "# print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(x_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8]\n",
      " [1]\n",
      " [9]\n",
      " ...\n",
      " [5]\n",
      " [9]\n",
      " [9]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "-59990.00000000001\n",
      "(60000, 10) [0.99359561 0.84545361 0.46470173 0.53080517 0.26132875 0.38718854\n",
      " 0.58506732 0.09596762 0.69085826 0.76676904]\n",
      "accuracy is:  0.15011666666666668\n",
      "loss for epoch  0  is:  41.635000129058426\n",
      "[[5]\n",
      " [5]\n",
      " [5]\n",
      " ...\n",
      " [5]\n",
      " [5]\n",
      " [5]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "-59989.99999999999\n",
      "(60000, 10) [27.69001251 27.94495759 25.09298358 28.62185457 27.56237093 26.59642753\n",
      " 28.0718432  26.38011257 27.1840102  26.3820183 ]\n",
      "accuracy is:  0.09665\n",
      "loss for epoch  1  is:  43.562134130263246\n",
      "[[1]\n",
      " [1]\n",
      " [5]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "-59990.00000000001\n",
      "(60000, 10) [46.74530519 45.56201311 41.69536974 47.03865151 45.22105546 45.17584214\n",
      " 46.69360477 41.92082172 45.02519218 44.90436844]\n",
      "accuracy is:  0.1258\n",
      "loss for epoch  2  is:  42.511461633970455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [76.35433579 75.1713449  71.30423977 76.64810527 74.83060009 74.78481212\n",
      " 76.30293103 71.53039014 74.63453344 74.51337894]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  3  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  4  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  5  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  6  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  7  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  8  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  9  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  10  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  11  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  12  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  13  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  14  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  15  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  16  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  17  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  18  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  19  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  20  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  21  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  22  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  23  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  24  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  25  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  26  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  27  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  28  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  29  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  30  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  31  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  32  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  33  is:  43.48764391325905\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]] [[3]\n",
      " [2]\n",
      " [7]\n",
      " ...\n",
      " [0]\n",
      " [9]\n",
      " [0]]\n",
      "nan\n",
      "(60000, 10) [nan nan nan nan nan nan nan nan nan nan]\n",
      "accuracy is:  0.09871666666666666\n",
      "loss for epoch  34  is:  43.48764391325905\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-7b17068d252f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m#class output of true labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0my_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m     \"\"\"\n\u001b[0;32m-> 1186\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# param = network_initialization(784, 10, x_train)\n",
    "# def fit_SGD(x_train, y_train, y_test, hyperparam, param):\n",
    "import copy\n",
    "loss_epoch_report = 0\n",
    "accuracy = 0\n",
    "loss1 = 0\n",
    "loss2 = 0\n",
    "learning_rate = 0.01 #hyperparam[\"learning_rate\"]\n",
    "# y_epoch = copy.deepcopy(y_train)\n",
    "# x_epoch = copy.deepcopy(x_train)\n",
    "for epoch in range(hyperparam[\"num_epochs\"]):\n",
    "\n",
    "    shuffling = np.arange(hyperparam[\"num_samples\"])\n",
    "    np.random.RandomState(seed=1).shuffle(shuffling)\n",
    "    x_epoch = x_train[shuffling]\n",
    "    y_epoch = y_train[shuffling]\n",
    "\n",
    "    #calculating xTw + b\n",
    "#     print(param[\"w1\"][0])\n",
    "    z1 = np.dot(x_epoch, param[\"w1\"]) # + param[\"b1\"]\n",
    "    a1 = np.tanh(z1) # 6000 * 10\n",
    "#     print(a1[0])\n",
    "  \n",
    "    z_out = np.dot(a1, param[\"w_out\"]) # + param[\"b_out\"]\n",
    "    a_out, a_out_prob = softmax(z_out)\n",
    "#     print(a_out_prob[0])\n",
    "\n",
    "#     z2 = np.dot(a1, param[\"w2\"]) # + param[\"b2\"]\n",
    "#     a2 = np.tanh(z2)\n",
    "\n",
    "\n",
    "#     z_out = np.dot(a2, param[\"w_out\"]) # + param[\"b_out\"]\n",
    "#     a_out, a_out_prob = softmax(z_out)\n",
    "\n",
    "\n",
    "    #softmax class output\n",
    "    y_hat = np.argmax(a_out, axis = 1)[:, np.newaxis]\n",
    "    \n",
    "    #class output of true labels\n",
    "    y_label = np.argmax(y_epoch, axis = 1)[:, np.newaxis]\n",
    "    print(y_hat, y_label)\n",
    "\n",
    "    #calculating accuracy of the model\n",
    "    accuracy = (y_hat == y_label).all(axis = 1).mean()\n",
    "    \n",
    "    #back_propagation calculation\n",
    "    #it is like calculating the layer values based on its output\n",
    "    # and its supposed output --> based on this we can calculate\n",
    "    # the weights change\n",
    "#     dz_out = a_out_prob - y_epoch #6000*10\n",
    "#     dw_out = np.matmul(a2.T, dz_out) #10*10\n",
    "    \n",
    "    dz_out = a_out_prob - y_epoch #6000*10\n",
    "    print(np.sum(dz_out))\n",
    "#     print(dz_out[0])\n",
    "#     print(np.sum(a1.T))\n",
    "    dw_out = np.dot(a1.T, dz_out) #10*10\n",
    "#     print(dw_out[0])\n",
    "\n",
    "    #now again, we have to calculate the layer values change based\n",
    "    # on its output and its supposed output --> then calculate the\n",
    "    # weights\n",
    "#     dz2 = np.dot(dz_out, param[\"w_out\"].T) * (1 - np.power(a2, 2))\n",
    "#     dw2 = np.dot(a1.T, dz2)  #784*10\n",
    "    \n",
    "    print(dz_out.shape, param[\"w_out\"].T[0])\n",
    "    dz1 = np.dot(dz_out, param[\"w_out\"].T) * a1 *(1 - a1)\n",
    "#     print(dz1[0])\n",
    "    dw1 = np.dot(x_epoch.T, dz1)\n",
    "#     dz1 = np.dot(dz2, param[\"w2\"].T) * (1 - np.power(a1, 2))\n",
    "#     dw1 = np.dot(x_epoch.T, dz1)\n",
    "#     print(dw1[0])\n",
    "#     print(x_epoch.T[0])\n",
    "    \n",
    "#     print(\"a\")\n",
    "    \n",
    "    param[\"w1\"] = param[\"w1\"] - learning_rate * dw1\n",
    "#     print(param[\"w1\"][0])\n",
    "#     param[\"w2\"] = param[\"w2\"] - learning_rate * dw2\n",
    "    param[\"w_out\"] = param[\"w_out\"] - learning_rate * dw_out\n",
    "    \n",
    "#     print(\"f\")\n",
    "    #calling logloss function\n",
    "    loss_epoch_report = logloss(y_epoch, a_out)\n",
    "\n",
    "    #calculating gradient of weight and bias\n",
    "#     weight_grad = np.dot(x_epoch.T, softmax_out - y_epoch)\n",
    "#     bias_grad = -(softmax_out - y_epoch)\n",
    "\n",
    "    #updating parameters\n",
    "#     param[\"weight\"] =  param[\"weight\"] - learning_rate * weight_grad\n",
    "#     param[\"bias\"] = param[\"bias\"] - (learning_rate * bias_grad)\n",
    "#     print(\"g\")\n",
    "    #report so far\n",
    "    print(\"accuracy is: \", accuracy)\n",
    "    print(\"loss for epoch \", epoch, \" is: \", loss_epoch_report)\n",
    "    \n",
    "    if epoch % 60 == 0:\n",
    "        learning_rate /= 2\n",
    "    loss2 = loss_epoch_report\n",
    "    loss_epoch_report = 0\n",
    "    accuracy = 0\n",
    "# return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
